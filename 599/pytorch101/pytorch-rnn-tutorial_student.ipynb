{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Sentiment Analysis\n",
    "Code example adapted from:\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations\n",
    "Use the following command to install PyTorch (recommended in an virtualenv)  \n",
    "Further supports can be found here: https://pytorch.org/\n",
    "```bash\n",
    "pip3 install torch torchvision torchtext spacy\n",
    "python3 -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "This notebook will walk you through building a model to predict sentiment (i.e. positive or negative) using PyTorch and its useful library TorchText.  \n",
    "We will use a widely-used sentiment analysis benchmarking dataset, [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/), for our sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of setups\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.optim as optim\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "TorchText of PyTorch has a member function, `Field`, which defines how the data is processed.  \n",
    "Our data consists of both the raw text of the review and the labeled sentiment, either \"pos\" or \"neg\".  \n",
    "\n",
    "For natural lanuages, we usually need to \"tokenize\" a sentence into separated words.  \n",
    "Here our tokenization is done with the [spaCy](https://spacy.io) tokenizer.  \n",
    "The default is splitting the text on spaces.  \n",
    "`LabelField` here specifically is used for handling labels.  \n",
    "\n",
    "References of TorchText for further reading can be found [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.Field(tokenize='spacy')\n",
    "label = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use imdb dataset\n",
    "train_data, test_data = datasets.IMDB.splits(text, label)\n",
    "print('Number of training examples: {}'.format(len(train_data)))\n",
    "print('Number of testing examples: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an exempler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(seed))\n",
    "print('Number of training examples: {}'.format(len(train_data)))\n",
    "print('Number of validation examples: {}'.format(len(valid_data)))\n",
    "print('Number of testing examples: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Representations\n",
    "Vocabulary of a natural language sentence is usually represented with to its one-hot representation.  \n",
    "An illustration of one-hot representation\n",
    "\n",
    "![](https://i.imgur.com/0o5Gdar.png)\n",
    "\n",
    "\n",
    "For unknown vocabulary we use _unknown_ or `<unk>` token.\n",
    "\n",
    "The following builds the vocabulary, only keeping the most common `max_size` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.build_vocab(train_data, max_size=25000)\n",
    "label.build_vocab(train_data)\n",
    "print(\"Unique tokens in TEXT vocabulary: {}\".format(len(text.vocab)))\n",
    "print(\"Unique tokens in LABEL vocabulary: {}\".format(len(label.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in text.vocab.freqs.most_common(20):\n",
    "    print (k, ':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text.vocab.itos[:10])\n",
    "print(label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare torch device and training data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=batch_size,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **recurrent neural network** (RNN), specifically its variant, **Long-Short Term Memory** (LSTM), as our model architecture.  \n",
    "The below is an illustration, with the model predicting zero, indicating a negative sentiment.  \n",
    "The initial hidden state, $h_0$, will be initialized as a zero tensor. \n",
    "\n",
    "![](assets/sentiment1.png)\n",
    "\n",
    "The `forward` method is called when we feed examples into our model.\n",
    "\n",
    "`x`, is a tensor of size _**[sentence length, batch size]**_.\n",
    "\n",
    "You may notice that this tensor should have another dimension due to the one-hot vectors, however PyTorch conveniently stores a one-hot vector as it's index value, i.e. the tensor representing a sentence is just a tensor of the indexes for each token in that sentence.\n",
    "\n",
    "**Word embedding** layer is needed to to get `word_embd`, which gives us a dense vector representation of our sentences. `word_embd` is a tensor of size _**[sentence length, batch size, embedding dim]**_. `word_embd` is then fed into our model.\n",
    "\n",
    "The LSTM Model returns 2 tensors, `output` of size _**[sentence length, batch size, hidden dim]**_ and `hidden` of `state` and `cell`.\n",
    "\n",
    "We feed the last output tensor, to a linear classification layer `fc` to produce a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, embd_dim, rnn_size, out_proj_dim):\n",
    "        super().__init__()\n",
    "        ############################\n",
    "        #### Start of Your Code ####\n",
    "        ############################\n",
    "        pass\n",
    "        ############################\n",
    "        ##### End of Your Code #####\n",
    "        ############################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ############################\n",
    "        #### Start of Your Code ####\n",
    "        ############################\n",
    "        pass\n",
    "        ############################\n",
    "        ##### End of Your Code #####\n",
    "        ############################\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(text.vocab)\n",
    "embd_dim = 100\n",
    "rnn_size = 256\n",
    "out_proj_dim = 1\n",
    "\n",
    "model = Model(input_dim, embd_dim, rnn_size, out_proj_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer and training objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put them to device using `.to`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        ############################\n",
    "        #### Start of Your Code ####\n",
    "        ############################\n",
    "        # important! to make all the gradients zero\n",
    "                \n",
    "        # forward pass\n",
    "        \n",
    "        # compute the loss\n",
    "        \n",
    "        # compute the accuracy\n",
    "        \n",
    "        # backward pass\n",
    "        \n",
    "        # optimizer makes one gradient update step\n",
    "        \n",
    "        # aggregate training statistics\n",
    "        \n",
    "        ############################\n",
    "        ##### End of Your Code #####\n",
    "        ############################\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ############################\n",
    "            #### Start of Your Code ####\n",
    "            ############################\n",
    "            pass\n",
    "            ############################\n",
    "            ##### End of Your Code #####\n",
    "            ############################\n",
    "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader, criterion)\n",
    "    \n",
    "    print('| Epoch: {} | Train Loss: {:.3f} | Train Acc: {:.2f}% | Val. Loss: {:.3f} | Val. Acc: {:.2f}% |'.format(\n",
    "           epoch+1, train_loss, train_acc*100, valid_loss, valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print('| Test Loss: {:.3f} | Test Acc: {:.2f}% |'.format(\n",
    "      test_loss, test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
